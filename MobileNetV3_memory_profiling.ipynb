{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from utils import memory_cost_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hswish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        out = x * F.relu6(x+3, inplace = True) /6\n",
    "\n",
    "        return out\n",
    "\n",
    "class hsigmoid(nn.Module):\n",
    "    def forward(self,x ):\n",
    "        out = F.relu6(x+3, inplace = True) / 6\n",
    "        \n",
    "        return out\n",
    "\n",
    "class SeModule(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size, reduction = 4):\n",
    "        super(SeModule, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_size, in_size // reduction, kernel_size = 1, stride = 1, padding = 0, bias = False),\n",
    "            nn.BatchNorm2d(in_size // reduction),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_size // reduction, in_size, kernel_size = 1, stride = 1, padding = 0, bias = False),\n",
    "            nn.BatchNorm2d(in_size),\n",
    "            hsigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.se(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, kernel_size, in_size, expand_size, out_size, nolinear, semodule, stride):\n",
    "        super(Block, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.se = semodule\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_size, expand_size, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(expand_size)\n",
    "        self.nolinear1 = nolinear\n",
    "        self.conv2 = nn.Conv2d(expand_size, expand_size, kernel_size=kernel_size, stride=stride, padding=kernel_size//2, groups=expand_size, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(expand_size)\n",
    "        self.nolinear2 = nolinear\n",
    "        self.conv3 = nn.Conv2d(expand_size, out_size, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_size)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride == 1 and in_size != out_size:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_size, out_size, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(out_size),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.nolinear1(self.bn1(self.conv1(x)))\n",
    "        out = self.nolinear2(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        if self.se != None:\n",
    "            out = self.se(out)\n",
    "        out = out + self.shortcut(x) if self.stride==1 else out\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_193171/313173020.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMobileNetV3_Large\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMobileNetV3_Large\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class MobileNetV3_Large(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(MobileNetV3_Large, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.hs1 = hswish()\n",
    "\n",
    "        self.bneck = nn.Sequential(\n",
    "            Block(3, 16, 16, 16, nn.ReLU(inplace=True), None, 1),\n",
    "            Block(3, 16, 64, 24, nn.ReLU(inplace=True), None, 2),\n",
    "            Block(3, 24, 72, 24, nn.ReLU(inplace=True), None, 1),\n",
    "            Block(5, 24, 72, 40, nn.ReLU(inplace=True), SeModule(40), 2),\n",
    "            Block(5, 40, 120, 40, nn.ReLU(inplace=True), SeModule(40), 1),\n",
    "            Block(5, 40, 120, 40, nn.ReLU(inplace=True), SeModule(40), 1),\n",
    "            Block(3, 40, 240, 80, hswish(), None, 2),\n",
    "            Block(3, 80, 200, 80, hswish(), None, 1),\n",
    "            Block(3, 80, 184, 80, hswish(), None, 1),\n",
    "            Block(3, 80, 184, 80, hswish(), None, 1),\n",
    "            Block(3, 80, 480, 112, hswish(), SeModule(112), 1),\n",
    "            Block(3, 112, 672, 112, hswish(), SeModule(112), 1),\n",
    "            Block(5, 112, 672, 160, hswish(), SeModule(160), 1),\n",
    "            Block(5, 160, 672, 160, hswish(), SeModule(160), 2),\n",
    "            Block(5, 160, 960, 160, hswish(), SeModule(160), 1),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.conv2 = nn.Conv2d(160, 960, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(960)\n",
    "        self.hs2 = hswish()\n",
    "        self.linear3 = nn.Linear(960, 1280)\n",
    "        self.bn3 = nn.BatchNorm1d(1280)\n",
    "        self.hs3 = hswish()\n",
    "        self.linear4 = nn.Linear(1280, num_classes)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.hs1(self.bn1(self.conv1(x)))\n",
    "        out = self.bneck(out)\n",
    "        out = self.hs2(self.bn2(self.conv2(out)))\n",
    "        out = F.avg_pool2d(out, 7)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.hs3(self.bn3(self.linear3(out)))\n",
    "        out = self.linear4(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class MobileNetV3_Small(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(MobileNetV3_Small, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.hs1 = hswish()\n",
    "\n",
    "        self.bneck = nn.Sequential(\n",
    "            Block(3, 16, 16, 16, nn.ReLU(inplace=True), SeModule(16), 2),\n",
    "            Block(3, 16, 72, 24, nn.ReLU(inplace=True), None, 2),\n",
    "            Block(3, 24, 88, 24, nn.ReLU(inplace=True), None, 1),\n",
    "            Block(5, 24, 96, 40, hswish(), SeModule(40), 2),\n",
    "            Block(5, 40, 240, 40, hswish(), SeModule(40), 1),\n",
    "            Block(5, 40, 240, 40, hswish(), SeModule(40), 1),\n",
    "            Block(5, 40, 120, 48, hswish(), SeModule(48), 1),\n",
    "            Block(5, 48, 144, 48, hswish(), SeModule(48), 1),\n",
    "            Block(5, 48, 288, 96, hswish(), SeModule(96), 2),\n",
    "            Block(5, 96, 576, 96, hswish(), SeModule(96), 1),\n",
    "            Block(5, 96, 576, 96, hswish(), SeModule(96), 1),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.conv2 = nn.Conv2d(96, 576, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(576)\n",
    "        self.hs2 = hswish()\n",
    "        self.linear3 = nn.Linear(576, 1280)\n",
    "        self.bn3 = nn.BatchNorm1d(1280)\n",
    "        self.hs3 = hswish()\n",
    "        self.linear4 = nn.Linear(1280, num_classes)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        out = self.hs1(self.bn1(self.conv1(x)))\n",
    "        out = self.bneck(out)\n",
    "        out = self.hs2(self.bn2(self.conv2(out)))\n",
    "        out = F.avg_pool2d(out, 7)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.hs3(self.bn3(self.linear3(out)))\n",
    "        out = self.linear4(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = MobileNetV3_Large()\n",
    "    x = torch.randn(2,3,224,224)\n",
    "    y = net(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNetV3_Large()\n",
    "inputs = torch.randn(2,3,224,224)\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "         aten::native_batch_norm        52.24%     261.553ms        54.33%     272.050ms       4.122ms            66  \n",
      "        aten::mkldnn_convolution        37.83%     189.400ms        37.92%     189.868ms       2.921ms            65  \n",
      "                 model_inference         3.44%      17.212ms        99.70%     499.194ms     499.194ms             1  \n",
      "                       aten::add         0.74%       3.721ms         0.88%       4.414ms      41.642us           106  \n",
      "                       aten::sum         0.73%       3.672ms         0.82%       4.098ms      55.378us            74  \n",
      "                       aten::mul         0.45%       2.278ms         0.45%       2.278ms      78.552us            29  \n",
      "                      aten::div_         0.44%       2.200ms         0.88%       4.387ms      59.284us            74  \n",
      "                     aten::copy_         0.42%       2.088ms         0.42%       2.088ms      12.810us           163  \n",
      "                       aten::div         0.41%       2.053ms         0.52%       2.593ms      89.414us            29  \n",
      "                  aten::_to_copy         0.33%       1.657ms         0.49%       2.432ms      18.424us           132  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 500.702ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W CPUAllocator.cpp:305] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n"
     ]
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU],\n",
    "        profile_memory=True, record_shapes=True) as prof:\n",
    "    model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     aten::empty         1.27%       6.147ms         1.27%       6.147ms      11.300us      74.46 Mb      74.46 Mb           544  \n",
      "                       aten::add         4.42%      21.373ms         4.57%      22.088ms     208.377us      16.41 Mb      16.41 Mb           106  \n",
      "                       aten::mul         2.95%      14.279ms         2.95%      14.279ms     492.379us      14.23 Mb      14.23 Mb            29  \n",
      "             aten::empty_strided         0.31%       1.495ms         0.31%       1.495ms       9.286us      12.83 Mb      12.83 Mb           161  \n",
      "                       aten::div         1.58%       7.651ms         1.73%       8.376ms     288.828us      12.83 Mb      12.83 Mb            29  \n",
      "                      aten::mean         0.37%       1.798ms         2.42%      11.693ms     158.014us      61.99 Kb      61.99 Kb            74  \n",
      "                     aten::addmm         2.27%      10.950ms         2.27%      10.979ms       5.489ms      17.81 Kb      17.81 Kb             2  \n",
      "                aten::avg_pool2d         0.01%      42.000us         0.01%      42.000us      42.000us       7.50 Kb       7.50 Kb             1  \n",
      "                    aten::conv2d         0.12%     587.000us        60.19%     290.848ms       4.475ms      36.99 Mb           0 b            65  \n",
      "               aten::convolution         0.18%     893.000us        60.07%     290.261ms       4.466ms      36.99 Mb           0 b            65  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 483.241ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 112, 112]             432\n",
      "       BatchNorm2d-2         [-1, 16, 112, 112]              32\n",
      "            hswish-3         [-1, 16, 112, 112]               0\n",
      "            Conv2d-4         [-1, 16, 112, 112]             256\n",
      "       BatchNorm2d-5         [-1, 16, 112, 112]              32\n",
      "              ReLU-6         [-1, 16, 112, 112]               0\n",
      "            Conv2d-7         [-1, 16, 112, 112]             144\n",
      "       BatchNorm2d-8         [-1, 16, 112, 112]              32\n",
      "              ReLU-9         [-1, 16, 112, 112]               0\n",
      "           Conv2d-10         [-1, 16, 112, 112]             256\n",
      "      BatchNorm2d-11         [-1, 16, 112, 112]              32\n",
      "            Block-12         [-1, 16, 112, 112]               0\n",
      "           Conv2d-13         [-1, 64, 112, 112]           1,024\n",
      "      BatchNorm2d-14         [-1, 64, 112, 112]             128\n",
      "             ReLU-15         [-1, 64, 112, 112]               0\n",
      "           Conv2d-16           [-1, 64, 56, 56]             576\n",
      "      BatchNorm2d-17           [-1, 64, 56, 56]             128\n",
      "             ReLU-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19           [-1, 24, 56, 56]           1,536\n",
      "      BatchNorm2d-20           [-1, 24, 56, 56]              48\n",
      "            Block-21           [-1, 24, 56, 56]               0\n",
      "           Conv2d-22           [-1, 72, 56, 56]           1,728\n",
      "      BatchNorm2d-23           [-1, 72, 56, 56]             144\n",
      "             ReLU-24           [-1, 72, 56, 56]               0\n",
      "           Conv2d-25           [-1, 72, 56, 56]             648\n",
      "      BatchNorm2d-26           [-1, 72, 56, 56]             144\n",
      "             ReLU-27           [-1, 72, 56, 56]               0\n",
      "           Conv2d-28           [-1, 24, 56, 56]           1,728\n",
      "      BatchNorm2d-29           [-1, 24, 56, 56]              48\n",
      "            Block-30           [-1, 24, 56, 56]               0\n",
      "           Conv2d-31           [-1, 72, 56, 56]           1,728\n",
      "      BatchNorm2d-32           [-1, 72, 56, 56]             144\n",
      "             ReLU-33           [-1, 72, 56, 56]               0\n",
      "           Conv2d-34           [-1, 72, 28, 28]           1,800\n",
      "      BatchNorm2d-35           [-1, 72, 28, 28]             144\n",
      "             ReLU-36           [-1, 72, 28, 28]               0\n",
      "           Conv2d-37           [-1, 40, 28, 28]           2,880\n",
      "      BatchNorm2d-38           [-1, 40, 28, 28]              80\n",
      "AdaptiveAvgPool2d-39             [-1, 40, 1, 1]               0\n",
      "           Conv2d-40             [-1, 10, 1, 1]             400\n",
      "      BatchNorm2d-41             [-1, 10, 1, 1]              20\n",
      "             ReLU-42             [-1, 10, 1, 1]               0\n",
      "           Conv2d-43             [-1, 40, 1, 1]             400\n",
      "      BatchNorm2d-44             [-1, 40, 1, 1]              80\n",
      "         hsigmoid-45             [-1, 40, 1, 1]               0\n",
      "         SeModule-46           [-1, 40, 28, 28]               0\n",
      "            Block-47           [-1, 40, 28, 28]               0\n",
      "           Conv2d-48          [-1, 120, 28, 28]           4,800\n",
      "      BatchNorm2d-49          [-1, 120, 28, 28]             240\n",
      "             ReLU-50          [-1, 120, 28, 28]               0\n",
      "           Conv2d-51          [-1, 120, 28, 28]           3,000\n",
      "      BatchNorm2d-52          [-1, 120, 28, 28]             240\n",
      "             ReLU-53          [-1, 120, 28, 28]               0\n",
      "           Conv2d-54           [-1, 40, 28, 28]           4,800\n",
      "      BatchNorm2d-55           [-1, 40, 28, 28]              80\n",
      "AdaptiveAvgPool2d-56             [-1, 40, 1, 1]               0\n",
      "           Conv2d-57             [-1, 10, 1, 1]             400\n",
      "      BatchNorm2d-58             [-1, 10, 1, 1]              20\n",
      "             ReLU-59             [-1, 10, 1, 1]               0\n",
      "           Conv2d-60             [-1, 40, 1, 1]             400\n",
      "      BatchNorm2d-61             [-1, 40, 1, 1]              80\n",
      "         hsigmoid-62             [-1, 40, 1, 1]               0\n",
      "         SeModule-63           [-1, 40, 28, 28]               0\n",
      "            Block-64           [-1, 40, 28, 28]               0\n",
      "           Conv2d-65          [-1, 120, 28, 28]           4,800\n",
      "      BatchNorm2d-66          [-1, 120, 28, 28]             240\n",
      "             ReLU-67          [-1, 120, 28, 28]               0\n",
      "           Conv2d-68          [-1, 120, 28, 28]           3,000\n",
      "      BatchNorm2d-69          [-1, 120, 28, 28]             240\n",
      "             ReLU-70          [-1, 120, 28, 28]               0\n",
      "           Conv2d-71           [-1, 40, 28, 28]           4,800\n",
      "      BatchNorm2d-72           [-1, 40, 28, 28]              80\n",
      "AdaptiveAvgPool2d-73             [-1, 40, 1, 1]               0\n",
      "           Conv2d-74             [-1, 10, 1, 1]             400\n",
      "      BatchNorm2d-75             [-1, 10, 1, 1]              20\n",
      "             ReLU-76             [-1, 10, 1, 1]               0\n",
      "           Conv2d-77             [-1, 40, 1, 1]             400\n",
      "      BatchNorm2d-78             [-1, 40, 1, 1]              80\n",
      "         hsigmoid-79             [-1, 40, 1, 1]               0\n",
      "         SeModule-80           [-1, 40, 28, 28]               0\n",
      "            Block-81           [-1, 40, 28, 28]               0\n",
      "           Conv2d-82          [-1, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-83          [-1, 240, 28, 28]             480\n",
      "           hswish-84          [-1, 240, 28, 28]               0\n",
      "           Conv2d-85          [-1, 240, 14, 14]           2,160\n",
      "      BatchNorm2d-86          [-1, 240, 14, 14]             480\n",
      "           hswish-87          [-1, 240, 14, 14]               0\n",
      "           Conv2d-88           [-1, 80, 14, 14]          19,200\n",
      "      BatchNorm2d-89           [-1, 80, 14, 14]             160\n",
      "            Block-90           [-1, 80, 14, 14]               0\n",
      "           Conv2d-91          [-1, 200, 14, 14]          16,000\n",
      "      BatchNorm2d-92          [-1, 200, 14, 14]             400\n",
      "           hswish-93          [-1, 200, 14, 14]               0\n",
      "           Conv2d-94          [-1, 200, 14, 14]           1,800\n",
      "      BatchNorm2d-95          [-1, 200, 14, 14]             400\n",
      "           hswish-96          [-1, 200, 14, 14]               0\n",
      "           Conv2d-97           [-1, 80, 14, 14]          16,000\n",
      "      BatchNorm2d-98           [-1, 80, 14, 14]             160\n",
      "            Block-99           [-1, 80, 14, 14]               0\n",
      "          Conv2d-100          [-1, 184, 14, 14]          14,720\n",
      "     BatchNorm2d-101          [-1, 184, 14, 14]             368\n",
      "          hswish-102          [-1, 184, 14, 14]               0\n",
      "          Conv2d-103          [-1, 184, 14, 14]           1,656\n",
      "     BatchNorm2d-104          [-1, 184, 14, 14]             368\n",
      "          hswish-105          [-1, 184, 14, 14]               0\n",
      "          Conv2d-106           [-1, 80, 14, 14]          14,720\n",
      "     BatchNorm2d-107           [-1, 80, 14, 14]             160\n",
      "           Block-108           [-1, 80, 14, 14]               0\n",
      "          Conv2d-109          [-1, 184, 14, 14]          14,720\n",
      "     BatchNorm2d-110          [-1, 184, 14, 14]             368\n",
      "          hswish-111          [-1, 184, 14, 14]               0\n",
      "          Conv2d-112          [-1, 184, 14, 14]           1,656\n",
      "     BatchNorm2d-113          [-1, 184, 14, 14]             368\n",
      "          hswish-114          [-1, 184, 14, 14]               0\n",
      "          Conv2d-115           [-1, 80, 14, 14]          14,720\n",
      "     BatchNorm2d-116           [-1, 80, 14, 14]             160\n",
      "           Block-117           [-1, 80, 14, 14]               0\n",
      "          Conv2d-118          [-1, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-119          [-1, 480, 14, 14]             960\n",
      "          hswish-120          [-1, 480, 14, 14]               0\n",
      "          Conv2d-121          [-1, 480, 14, 14]           4,320\n",
      "     BatchNorm2d-122          [-1, 480, 14, 14]             960\n",
      "          hswish-123          [-1, 480, 14, 14]               0\n",
      "          Conv2d-124          [-1, 112, 14, 14]          53,760\n",
      "     BatchNorm2d-125          [-1, 112, 14, 14]             224\n",
      "AdaptiveAvgPool2d-126            [-1, 112, 1, 1]               0\n",
      "          Conv2d-127             [-1, 28, 1, 1]           3,136\n",
      "     BatchNorm2d-128             [-1, 28, 1, 1]              56\n",
      "            ReLU-129             [-1, 28, 1, 1]               0\n",
      "          Conv2d-130            [-1, 112, 1, 1]           3,136\n",
      "     BatchNorm2d-131            [-1, 112, 1, 1]             224\n",
      "        hsigmoid-132            [-1, 112, 1, 1]               0\n",
      "        SeModule-133          [-1, 112, 14, 14]               0\n",
      "          Conv2d-134          [-1, 112, 14, 14]           8,960\n",
      "     BatchNorm2d-135          [-1, 112, 14, 14]             224\n",
      "           Block-136          [-1, 112, 14, 14]               0\n",
      "          Conv2d-137          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-138          [-1, 672, 14, 14]           1,344\n",
      "          hswish-139          [-1, 672, 14, 14]               0\n",
      "          Conv2d-140          [-1, 672, 14, 14]           6,048\n",
      "     BatchNorm2d-141          [-1, 672, 14, 14]           1,344\n",
      "          hswish-142          [-1, 672, 14, 14]               0\n",
      "          Conv2d-143          [-1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-144          [-1, 112, 14, 14]             224\n",
      "AdaptiveAvgPool2d-145            [-1, 112, 1, 1]               0\n",
      "          Conv2d-146             [-1, 28, 1, 1]           3,136\n",
      "     BatchNorm2d-147             [-1, 28, 1, 1]              56\n",
      "            ReLU-148             [-1, 28, 1, 1]               0\n",
      "          Conv2d-149            [-1, 112, 1, 1]           3,136\n",
      "     BatchNorm2d-150            [-1, 112, 1, 1]             224\n",
      "        hsigmoid-151            [-1, 112, 1, 1]               0\n",
      "        SeModule-152          [-1, 112, 14, 14]               0\n",
      "           Block-153          [-1, 112, 14, 14]               0\n",
      "          Conv2d-154          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-155          [-1, 672, 14, 14]           1,344\n",
      "          hswish-156          [-1, 672, 14, 14]               0\n",
      "          Conv2d-157          [-1, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-158          [-1, 672, 14, 14]           1,344\n",
      "          hswish-159          [-1, 672, 14, 14]               0\n",
      "          Conv2d-160          [-1, 160, 14, 14]         107,520\n",
      "     BatchNorm2d-161          [-1, 160, 14, 14]             320\n",
      "AdaptiveAvgPool2d-162            [-1, 160, 1, 1]               0\n",
      "          Conv2d-163             [-1, 40, 1, 1]           6,400\n",
      "     BatchNorm2d-164             [-1, 40, 1, 1]              80\n",
      "            ReLU-165             [-1, 40, 1, 1]               0\n",
      "          Conv2d-166            [-1, 160, 1, 1]           6,400\n",
      "     BatchNorm2d-167            [-1, 160, 1, 1]             320\n",
      "        hsigmoid-168            [-1, 160, 1, 1]               0\n",
      "        SeModule-169          [-1, 160, 14, 14]               0\n",
      "          Conv2d-170          [-1, 160, 14, 14]          17,920\n",
      "     BatchNorm2d-171          [-1, 160, 14, 14]             320\n",
      "           Block-172          [-1, 160, 14, 14]               0\n",
      "          Conv2d-173          [-1, 672, 14, 14]         107,520\n",
      "     BatchNorm2d-174          [-1, 672, 14, 14]           1,344\n",
      "          hswish-175          [-1, 672, 14, 14]               0\n",
      "          Conv2d-176            [-1, 672, 7, 7]          16,800\n",
      "     BatchNorm2d-177            [-1, 672, 7, 7]           1,344\n",
      "          hswish-178            [-1, 672, 7, 7]               0\n",
      "          Conv2d-179            [-1, 160, 7, 7]         107,520\n",
      "     BatchNorm2d-180            [-1, 160, 7, 7]             320\n",
      "AdaptiveAvgPool2d-181            [-1, 160, 1, 1]               0\n",
      "          Conv2d-182             [-1, 40, 1, 1]           6,400\n",
      "     BatchNorm2d-183             [-1, 40, 1, 1]              80\n",
      "            ReLU-184             [-1, 40, 1, 1]               0\n",
      "          Conv2d-185            [-1, 160, 1, 1]           6,400\n",
      "     BatchNorm2d-186            [-1, 160, 1, 1]             320\n",
      "        hsigmoid-187            [-1, 160, 1, 1]               0\n",
      "        SeModule-188            [-1, 160, 7, 7]               0\n",
      "           Block-189            [-1, 160, 7, 7]               0\n",
      "          Conv2d-190            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-191            [-1, 960, 7, 7]           1,920\n",
      "          hswish-192            [-1, 960, 7, 7]               0\n",
      "          Conv2d-193            [-1, 960, 7, 7]          24,000\n",
      "     BatchNorm2d-194            [-1, 960, 7, 7]           1,920\n",
      "          hswish-195            [-1, 960, 7, 7]               0\n",
      "          Conv2d-196            [-1, 160, 7, 7]         153,600\n",
      "     BatchNorm2d-197            [-1, 160, 7, 7]             320\n",
      "AdaptiveAvgPool2d-198            [-1, 160, 1, 1]               0\n",
      "          Conv2d-199             [-1, 40, 1, 1]           6,400\n",
      "     BatchNorm2d-200             [-1, 40, 1, 1]              80\n",
      "            ReLU-201             [-1, 40, 1, 1]               0\n",
      "          Conv2d-202            [-1, 160, 1, 1]           6,400\n",
      "     BatchNorm2d-203            [-1, 160, 1, 1]             320\n",
      "        hsigmoid-204            [-1, 160, 1, 1]               0\n",
      "        SeModule-205            [-1, 160, 7, 7]               0\n",
      "           Block-206            [-1, 160, 7, 7]               0\n",
      "          Conv2d-207            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-208            [-1, 960, 7, 7]           1,920\n",
      "          hswish-209            [-1, 960, 7, 7]               0\n",
      "          Linear-210                 [-1, 1280]       1,230,080\n",
      "     BatchNorm1d-211                 [-1, 1280]           2,560\n",
      "          hswish-212                 [-1, 1280]               0\n",
      "          Linear-213                 [-1, 1000]       1,281,000\n",
      "================================================================\n",
      "Total params: 3,955,916\n",
      "Trainable params: 3,955,916\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 112.00\n",
      "Params size (MB): 15.09\n",
      "Estimated Total Size (MB): 127.67\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model, (3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConv2d(nn.Conv2d):\n",
    "\t\"\"\"\n",
    "\tConv2d with Weight Standardization\n",
    "\thttps://github.com/joe-siyuan-qiao/WeightStandardization\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "\t             padding=0, dilation=1, groups=1, bias=True):\n",
    "\t\tsuper(MyConv2d, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
    "\t\tself.WS_EPS = None\n",
    "\n",
    "\tdef weight_standardization(self, weight):\n",
    "\t\tif self.WS_EPS is not None:\n",
    "\t\t\tweight_mean = weight.mean(dim=1, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "\t\t\tweight = weight - weight_mean\n",
    "\t\t\tstd = weight.view(weight.size(0), -1).std(dim=1).view(-1, 1, 1, 1) + self.WS_EPS\n",
    "\t\t\tweight = weight / std.expand_as(weight)\n",
    "\t\treturn weight\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tif self.WS_EPS is None:\n",
    "\t\t\treturn super(MyConv2d, self).forward(x)\n",
    "\t\telse:\n",
    "\t\t\treturn F.conv2d(x, self.weight_standardization(self.weight), self.bias,\n",
    "\t\t\t                self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn super(MyConv2d, self).__repr__()[:-1] + ', ws_eps=%s)' % self.WS_EPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hswish(nn.Module):\n",
    "\n",
    "\tdef __init__(self, inplace=True):\n",
    "\t\tsuper(Hswish, self).__init__()\n",
    "\t\tself.inplace = inplace\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn x * F.relu6(x + 3., inplace=self.inplace) / 6.\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn 'Hswish()'\n",
    "\n",
    "\n",
    "class Hsigmoid(nn.Module):\n",
    "\n",
    "\tdef __init__(self, inplace=True):\n",
    "\t\tsuper(Hsigmoid, self).__init__()\n",
    "\t\tself.inplace = inplace\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn F.relu6(x + 3., inplace=self.inplace) / 6.\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn 'Hsigmoid()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with profile(activities= [ProfilerActivity.CPU], record_shapes= True) as prof:\n",
    "    model(inputs)\n",
    "\n",
    "prof.export_chrome_trace(\"trace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     aten::empty         2.23%       5.833ms         2.23%       5.833ms      10.722us      74.46 Mb      74.46 Mb           544  \n",
      "                       aten::add         2.27%       5.943ms         2.57%       6.715ms      63.349us      16.41 Mb      16.41 Mb           106  \n",
      "                       aten::mul         0.91%       2.379ms         0.91%       2.379ms      82.034us      14.23 Mb      14.23 Mb            29  \n",
      "             aten::empty_strided         0.57%       1.485ms         0.57%       1.485ms       9.224us      12.83 Mb      12.83 Mb           161  \n",
      "                       aten::div         0.91%       2.391ms         1.21%       3.158ms     108.897us      12.83 Mb      12.83 Mb            29  \n",
      "                      aten::mean         0.75%       1.970ms         4.56%      11.928ms     161.189us      61.99 Kb      61.99 Kb            74  \n",
      "                     aten::addmm         1.19%       3.112ms         1.20%       3.148ms       1.574ms      17.81 Kb      17.81 Kb             2  \n",
      "                aten::avg_pool2d         0.02%      47.000us         0.02%      47.000us      47.000us       7.50 Kb       7.50 Kb             1  \n",
      "                    aten::conv2d         0.29%     748.000us        65.66%     171.886ms       2.644ms      36.99 Mb           0 b            65  \n",
      "               aten::convolution         0.36%     939.000us        65.37%     171.138ms       2.633ms      36.99 Mb           0 b            65  \n",
      "              aten::_convolution         0.44%       1.161ms        65.02%     170.199ms       2.618ms      36.99 Mb           0 b            65  \n",
      "        aten::mkldnn_convolution        64.18%     168.003ms        64.57%     169.038ms       2.601ms      36.99 Mb           0 b            65  \n",
      "               aten::as_strided_         0.05%     119.000us         0.05%     119.000us       1.831us           0 b           0 b            65  \n",
      "                aten::batch_norm         0.22%     580.000us        26.25%      68.709ms       1.041ms      37.10 Mb           0 b            66  \n",
      "    aten::_batch_norm_impl_index         0.45%       1.168ms        26.03%      68.129ms       1.032ms      37.10 Mb           0 b            66  \n",
      "                       aten::sum         1.51%       3.948ms         1.67%       4.382ms      59.216us           0 b           0 b            74  \n",
      "                aten::as_strided         0.08%     203.000us         0.08%     203.000us       2.900us           0 b           0 b            70  \n",
      "                     aten::fill_         0.09%     240.000us         0.09%     240.000us       3.243us           0 b           0 b            74  \n",
      "                        aten::to         0.29%     760.000us         1.57%       4.105ms      31.098us         528 b           0 b           132  \n",
      "                  aten::_to_copy         0.63%       1.648ms         1.28%       3.345ms      25.341us         528 b           0 b           132  \n",
      "                     aten::copy_         0.88%       2.298ms         0.88%       2.298ms      14.098us           0 b           0 b           163  \n",
      "                aten::empty_like         0.19%     495.000us         0.49%       1.291ms      19.561us      36.99 Mb           0 b            66  \n",
      "                    aten::relu6_         0.06%     163.000us         1.36%       3.557ms     122.655us      12.83 Mb           0 b            29  \n",
      "                 aten::hardtanh_         0.19%     499.000us         1.30%       3.394ms     117.034us      12.83 Mb           0 b            29  \n",
      "                     aten::clone         0.16%     428.000us         0.95%       2.499ms      86.172us      12.83 Mb           0 b            29  \n",
      "                    aten::clamp_         0.15%     396.000us         0.15%     396.000us      13.655us           0 b           0 b            29  \n",
      "                     aten::relu_         0.10%     255.000us         0.42%       1.111ms      55.550us           0 b           0 b            20  \n",
      "                aten::clamp_min_         0.05%     126.000us         0.33%     856.000us      42.800us           0 b           0 b            20  \n",
      "                 aten::clamp_min         0.28%     730.000us         0.28%     730.000us      36.500us           0 b           0 b            20  \n",
      "       aten::adaptive_avg_pool2d         0.02%      51.000us         0.36%     950.000us     118.750us       6.44 Kb           0 b             8  \n",
      "                     aten::zero_         0.05%     126.000us         0.05%     126.000us       3.706us           0 b           0 b            34  \n",
      "                      aten::view         0.01%      14.000us         0.01%      14.000us      14.000us           0 b           0 b             1  \n",
      "                    aten::linear         0.01%      39.000us         1.24%       3.254ms       1.627ms      17.81 Kb           0 b             2  \n",
      "                         aten::t         0.02%      44.000us         0.03%      67.000us      33.500us           0 b           0 b             2  \n",
      "                 aten::transpose         0.01%      18.000us         0.01%      23.000us      11.500us           0 b           0 b             2  \n",
      "                    aten::expand         0.01%      15.000us         0.01%      19.000us       9.500us           0 b           0 b             2  \n",
      "              aten::resolve_conj         0.00%       2.000us         0.00%       2.000us       0.500us           0 b           0 b             4  \n",
      "                      aten::div_         1.15%       3.010ms         2.13%       5.576ms      75.351us           0 b        -296 b            74  \n",
      "         aten::native_batch_norm        19.25%      50.394ms        25.51%      66.781ms       1.012ms      37.10 Mb    -438.78 Kb            66  \n",
      "                        [memory]         0.00%       0.000us         0.00%       0.000us       0.000us    -130.42 Mb    -130.42 Mb           649  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 261.780ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU], profile_memory = True, record_shapes = True) as prof:\n",
    "    model(inputs)\n",
    "\n",
    "print(prof.key_averages(group_by_stack_n = 10).table(sort_by = \"self_cpu_memory_usage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_handler(prof):\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by = \"self_cpu_memory_usage\", row_limit = -1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU\n",
    "    ],\n",
    "\n",
    "    # In this example with wait=1, warmup=1, active=2,\n",
    "    # profiler will skip the first step/iteration,\n",
    "    # start warming up on the second, record\n",
    "    # the third and the forth iterations,\n",
    "    # after which the trace will become available\n",
    "    # and on_trace_ready (when set) is called;\n",
    "    # the cycle repeats starting with the next step\n",
    "\n",
    "    schedule=torch.profiler.schedule(\n",
    "        wait=1,\n",
    "        warmup=1,\n",
    "        active=2),\n",
    "    \n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')\n",
    "\n",
    "    ) as p:\n",
    "    model(inputs)\n",
    "\n",
    "#print(p.key_averages(group_by_input_shape= True).table(sort_by= \"self_cpu_memory_usage\",row_limit = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
